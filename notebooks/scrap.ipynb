{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-26 10:47:59,864] A new study created in memory with name: no-name-5b53daa7-b99a-4456-bd3b-418695a5d2e8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A new study created in memory with name: no-name-5b53daa7-b99a-4456-bd3b-418695a5d2e8\n",
      "A new study created in memory with name: no-name-5b53daa7-b99a-4456-bd3b-418695a5d2e8\n",
      "A new study created in memory with name: no-name-5b53daa7-b99a-4456-bd3b-418695a5d2e8\n",
      "A new study created in memory with name: no-name-5b53daa7-b99a-4456-bd3b-418695a5d2e8\n",
      "A new study created in memory with name: no-name-5b53daa7-b99a-4456-bd3b-418695a5d2e8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-26 10:47:59,948] Trial 0 finished with value: 0.21052631578947367 and parameters: {'alpha': 0.034841676083516224}. Best is trial 0 with value: 0.21052631578947367.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0 finished with value: 0.21052631578947367 and parameters: {'alpha': 0.034841676083516224}. Best is trial 0 with value: 0.21052631578947367.\n",
      "Trial 0 finished with value: 0.21052631578947367 and parameters: {'alpha': 0.034841676083516224}. Best is trial 0 with value: 0.21052631578947367.\n",
      "Trial 0 finished with value: 0.21052631578947367 and parameters: {'alpha': 0.034841676083516224}. Best is trial 0 with value: 0.21052631578947367.\n",
      "Trial 0 finished with value: 0.21052631578947367 and parameters: {'alpha': 0.034841676083516224}. Best is trial 0 with value: 0.21052631578947367.\n",
      "Trial 0 finished with value: 0.21052631578947367 and parameters: {'alpha': 0.034841676083516224}. Best is trial 0 with value: 0.21052631578947367.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-26 10:48:00,030] Trial 1 finished with value: 0.07894736842105265 and parameters: {'alpha': 5.024988407245832e-05}. Best is trial 1 with value: 0.07894736842105265.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 finished with value: 0.07894736842105265 and parameters: {'alpha': 5.024988407245832e-05}. Best is trial 1 with value: 0.07894736842105265.\n",
      "Trial 1 finished with value: 0.07894736842105265 and parameters: {'alpha': 5.024988407245832e-05}. Best is trial 1 with value: 0.07894736842105265.\n",
      "Trial 1 finished with value: 0.07894736842105265 and parameters: {'alpha': 5.024988407245832e-05}. Best is trial 1 with value: 0.07894736842105265.\n",
      "Trial 1 finished with value: 0.07894736842105265 and parameters: {'alpha': 5.024988407245832e-05}. Best is trial 1 with value: 0.07894736842105265.\n",
      "Trial 1 finished with value: 0.07894736842105265 and parameters: {'alpha': 5.024988407245832e-05}. Best is trial 1 with value: 0.07894736842105265.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-26 10:48:00,112] Trial 2 finished with value: 0.052631578947368474 and parameters: {'alpha': 0.008605347915629629}. Best is trial 2 with value: 0.052631578947368474.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 finished with value: 0.052631578947368474 and parameters: {'alpha': 0.008605347915629629}. Best is trial 2 with value: 0.052631578947368474.\n",
      "Trial 2 finished with value: 0.052631578947368474 and parameters: {'alpha': 0.008605347915629629}. Best is trial 2 with value: 0.052631578947368474.\n",
      "Trial 2 finished with value: 0.052631578947368474 and parameters: {'alpha': 0.008605347915629629}. Best is trial 2 with value: 0.052631578947368474.\n",
      "Trial 2 finished with value: 0.052631578947368474 and parameters: {'alpha': 0.008605347915629629}. Best is trial 2 with value: 0.052631578947368474.\n",
      "Trial 2 finished with value: 0.052631578947368474 and parameters: {'alpha': 0.008605347915629629}. Best is trial 2 with value: 0.052631578947368474.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-26 10:48:00,194] Trial 3 finished with value: 0.052631578947368474 and parameters: {'alpha': 0.0002930657202688273}. Best is trial 2 with value: 0.052631578947368474.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 finished with value: 0.052631578947368474 and parameters: {'alpha': 0.0002930657202688273}. Best is trial 2 with value: 0.052631578947368474.\n",
      "Trial 3 finished with value: 0.052631578947368474 and parameters: {'alpha': 0.0002930657202688273}. Best is trial 2 with value: 0.052631578947368474.\n",
      "Trial 3 finished with value: 0.052631578947368474 and parameters: {'alpha': 0.0002930657202688273}. Best is trial 2 with value: 0.052631578947368474.\n",
      "Trial 3 finished with value: 0.052631578947368474 and parameters: {'alpha': 0.0002930657202688273}. Best is trial 2 with value: 0.052631578947368474.\n",
      "Trial 3 finished with value: 0.052631578947368474 and parameters: {'alpha': 0.0002930657202688273}. Best is trial 2 with value: 0.052631578947368474.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-26 10:48:00,301] Trial 4 finished with value: 0.052631578947368474 and parameters: {'alpha': 1.6514214919262076e-05}. Best is trial 2 with value: 0.052631578947368474.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4 finished with value: 0.052631578947368474 and parameters: {'alpha': 1.6514214919262076e-05}. Best is trial 2 with value: 0.052631578947368474.\n",
      "Trial 4 finished with value: 0.052631578947368474 and parameters: {'alpha': 1.6514214919262076e-05}. Best is trial 2 with value: 0.052631578947368474.\n",
      "Trial 4 finished with value: 0.052631578947368474 and parameters: {'alpha': 1.6514214919262076e-05}. Best is trial 2 with value: 0.052631578947368474.\n",
      "Trial 4 finished with value: 0.052631578947368474 and parameters: {'alpha': 1.6514214919262076e-05}. Best is trial 2 with value: 0.052631578947368474.\n",
      "Trial 4 finished with value: 0.052631578947368474 and parameters: {'alpha': 1.6514214919262076e-05}. Best is trial 2 with value: 0.052631578947368474.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-26 10:48:00,384] Trial 5 finished with value: 0.2894736842105263 and parameters: {'alpha': 0.04950249118875089}. Best is trial 2 with value: 0.052631578947368474.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 finished with value: 0.2894736842105263 and parameters: {'alpha': 0.04950249118875089}. Best is trial 2 with value: 0.052631578947368474.\n",
      "Trial 5 finished with value: 0.2894736842105263 and parameters: {'alpha': 0.04950249118875089}. Best is trial 2 with value: 0.052631578947368474.\n",
      "Trial 5 finished with value: 0.2894736842105263 and parameters: {'alpha': 0.04950249118875089}. Best is trial 2 with value: 0.052631578947368474.\n",
      "Trial 5 finished with value: 0.2894736842105263 and parameters: {'alpha': 0.04950249118875089}. Best is trial 2 with value: 0.052631578947368474.\n",
      "Trial 5 finished with value: 0.2894736842105263 and parameters: {'alpha': 0.04950249118875089}. Best is trial 2 with value: 0.052631578947368474.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-26 10:48:00,387] Trial 6 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6 pruned. \n",
      "Trial 6 pruned. \n",
      "Trial 6 pruned. \n",
      "Trial 6 pruned. \n",
      "Trial 6 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-26 10:48:00,390] Trial 7 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7 pruned. \n",
      "Trial 7 pruned. \n",
      "Trial 7 pruned. \n",
      "Trial 7 pruned. \n",
      "Trial 7 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-26 10:48:00,394] Trial 8 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8 pruned. \n",
      "Trial 8 pruned. \n",
      "Trial 8 pruned. \n",
      "Trial 8 pruned. \n",
      "Trial 8 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-26 10:48:00,477] Trial 9 finished with value: 0.39473684210526316 and parameters: {'alpha': 1.2331981507790151e-05}. Best is trial 2 with value: 0.052631578947368474.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9 finished with value: 0.39473684210526316 and parameters: {'alpha': 1.2331981507790151e-05}. Best is trial 2 with value: 0.052631578947368474.\n",
      "Trial 9 finished with value: 0.39473684210526316 and parameters: {'alpha': 1.2331981507790151e-05}. Best is trial 2 with value: 0.052631578947368474.\n",
      "Trial 9 finished with value: 0.39473684210526316 and parameters: {'alpha': 1.2331981507790151e-05}. Best is trial 2 with value: 0.052631578947368474.\n",
      "Trial 9 finished with value: 0.39473684210526316 and parameters: {'alpha': 1.2331981507790151e-05}. Best is trial 2 with value: 0.052631578947368474.\n",
      "Trial 9 finished with value: 0.39473684210526316 and parameters: {'alpha': 1.2331981507790151e-05}. Best is trial 2 with value: 0.052631578947368474.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-26 10:48:00,561] Trial 10 finished with value: 0.07894736842105265 and parameters: {'alpha': 0.003058178743420022}. Best is trial 2 with value: 0.052631578947368474.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 finished with value: 0.07894736842105265 and parameters: {'alpha': 0.003058178743420022}. Best is trial 2 with value: 0.052631578947368474.\n",
      "Trial 10 finished with value: 0.07894736842105265 and parameters: {'alpha': 0.003058178743420022}. Best is trial 2 with value: 0.052631578947368474.\n",
      "Trial 10 finished with value: 0.07894736842105265 and parameters: {'alpha': 0.003058178743420022}. Best is trial 2 with value: 0.052631578947368474.\n",
      "Trial 10 finished with value: 0.07894736842105265 and parameters: {'alpha': 0.003058178743420022}. Best is trial 2 with value: 0.052631578947368474.\n",
      "Trial 10 finished with value: 0.07894736842105265 and parameters: {'alpha': 0.003058178743420022}. Best is trial 2 with value: 0.052631578947368474.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-26 10:48:00,566] Trial 11 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 11 pruned. \n",
      "Trial 11 pruned. \n",
      "Trial 11 pruned. \n",
      "Trial 11 pruned. \n",
      "Trial 11 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-26 10:48:00,570] Trial 12 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 12 pruned. \n",
      "Trial 12 pruned. \n",
      "Trial 12 pruned. \n",
      "Trial 12 pruned. \n",
      "Trial 12 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-26 10:48:00,574] Trial 13 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 13 pruned. \n",
      "Trial 13 pruned. \n",
      "Trial 13 pruned. \n",
      "Trial 13 pruned. \n",
      "Trial 13 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-26 10:48:00,579] Trial 14 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 14 pruned. \n",
      "Trial 14 pruned. \n",
      "Trial 14 pruned. \n",
      "Trial 14 pruned. \n",
      "Trial 14 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-26 10:48:00,583] Trial 15 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 15 pruned. \n",
      "Trial 15 pruned. \n",
      "Trial 15 pruned. \n",
      "Trial 15 pruned. \n",
      "Trial 15 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-26 10:48:00,587] Trial 16 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 16 pruned. \n",
      "Trial 16 pruned. \n",
      "Trial 16 pruned. \n",
      "Trial 16 pruned. \n",
      "Trial 16 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-26 10:48:00,591] Trial 17 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 17 pruned. \n",
      "Trial 17 pruned. \n",
      "Trial 17 pruned. \n",
      "Trial 17 pruned. \n",
      "Trial 17 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-26 10:48:00,675] Trial 18 finished with value: 0.02631578947368418 and parameters: {'alpha': 0.0011422529345791776}. Best is trial 18 with value: 0.02631578947368418.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 18 finished with value: 0.02631578947368418 and parameters: {'alpha': 0.0011422529345791776}. Best is trial 18 with value: 0.02631578947368418.\n",
      "Trial 18 finished with value: 0.02631578947368418 and parameters: {'alpha': 0.0011422529345791776}. Best is trial 18 with value: 0.02631578947368418.\n",
      "Trial 18 finished with value: 0.02631578947368418 and parameters: {'alpha': 0.0011422529345791776}. Best is trial 18 with value: 0.02631578947368418.\n",
      "Trial 18 finished with value: 0.02631578947368418 and parameters: {'alpha': 0.0011422529345791776}. Best is trial 18 with value: 0.02631578947368418.\n",
      "Trial 18 finished with value: 0.02631578947368418 and parameters: {'alpha': 0.0011422529345791776}. Best is trial 18 with value: 0.02631578947368418.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-26 10:48:00,680] Trial 19 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 19 pruned. \n",
      "Trial 19 pruned. \n",
      "Trial 19 pruned. \n",
      "Trial 19 pruned. \n",
      "Trial 19 pruned. \n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "import sklearn.model_selection\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    iris = sklearn.datasets.load_iris()\n",
    "    classes = list(set(iris.target))\n",
    "    train_x, valid_x, train_y, valid_y = sklearn.model_selection.train_test_split(\n",
    "        iris.data, iris.target, test_size=0.25, random_state=0\n",
    "    )\n",
    "\n",
    "    alpha = trial.suggest_float(\"alpha\", 1e-5, 1e-1, log=True)\n",
    "    clf = sklearn.linear_model.SGDClassifier(alpha=alpha)\n",
    "\n",
    "    for step in range(100):\n",
    "        clf.partial_fit(train_x, train_y, classes=classes)\n",
    "\n",
    "        # Report intermediate objective value.\n",
    "        intermediate_value = 1.0 - clf.score(valid_x, valid_y)\n",
    "        trial.report(intermediate_value, step)\n",
    "\n",
    "        # Handle pruning based on the intermediate value.\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    return 1.0 - clf.score(valid_x, valid_y)\n",
    "\n",
    "# Add stream handler of stdout to show the messages\n",
    "optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\n",
    "study = optuna.create_study(pruner=optuna.pruners.MedianPruner())\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "del study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "study = optuna.create_study() # default\n",
    "print(f\"Sampler is {study.sampler.__class__.__name__}\")\n",
    "\n",
    "study = optuna.create_study(sampler=optuna.samplers.RandomSampler())\n",
    "print(f\"Sampler is {study.sampler.__class__.__name__}\")\n",
    "\n",
    "study = optuna.create_study(sampler=optuna.samplers.CmaEsSampler())\n",
    "print(f\"Sampler is {study.sampler.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo predict model=yolov8n.pt source='https://ultralytics.com/images/zidane.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load image\n",
    "image = cv2.imread('your_image.jpg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Convert image from RGB to HSV\n",
    "hsv_image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "\n",
    "# Function to adjust Hue, Saturation, or Value independently\n",
    "def adjust_hsv(hsv_img, h_shift=0, s_scale=1, v_scale=1):\n",
    "    hsv_copy = hsv_img.copy().astype(np.float32)\n",
    "    \n",
    "    # Adjust Hue\n",
    "    hsv_copy[..., 0] = (hsv_copy[..., 0] + h_shift) % 180\n",
    "    \n",
    "    # Adjust Saturation\n",
    "    hsv_copy[..., 1] = hsv_copy[..., 1] * s_scale\n",
    "    hsv_copy[..., 1] = np.clip(hsv_copy[..., 1], 0, 255)\n",
    "    \n",
    "    # Adjust Value\n",
    "    hsv_copy[..., 2] = hsv_copy[..., 2] * v_scale\n",
    "    hsv_copy[..., 2] = np.clip(hsv_copy[..., 2], 0, 255)\n",
    "    \n",
    "    # Convert back to uint8\n",
    "    hsv_copy = hsv_copy.astype(np.uint8)\n",
    "    \n",
    "    # Convert back to RGB for visualization\n",
    "    return cv2.cvtColor(hsv_copy, cv2.COLOR_HSV2RGB)\n",
    "\n",
    "# Create subplots for visualization\n",
    "fig, ax = plt.subplots(3, 4, figsize=(15, 10))\n",
    "\n",
    "# Original image\n",
    "ax[0, 0].imshow(image)\n",
    "ax[0, 0].set_title(\"Original\")\n",
    "ax[0, 0].axis('off')\n",
    "\n",
    "# Hue adjustments (shift hue)\n",
    "for i, h_shift in enumerate([30, 60, 90]):\n",
    "    ax[0, i+1].imshow(adjust_hsv(hsv_image, h_shift=h_shift))\n",
    "    ax[0, i+1].set_title(f'Hue Shift: {h_shift}')\n",
    "    ax[0, i+1].axis('off')\n",
    "\n",
    "# Saturation adjustments (scale saturation)\n",
    "for i, s_scale in enumerate([0.5, 1.5, 2]):\n",
    "    ax[1, i+1].imshow(adjust_hsv(hsv_image, s_scale=s_scale))\n",
    "    ax[1, i+1].set_title(f'Saturation x {s_scale}')\n",
    "    ax[1, i+1].axis('off')\n",
    "\n",
    "# Value adjustments (scale brightness)\n",
    "for i, v_scale in enumerate([0.5, 1.5, 2]):\n",
    "    ax[2, i+1].imshow(adjust_hsv(hsv_image, v_scale=v_scale))\n",
    "    ax[2, i+1].set_title(f'Value x {v_scale}')\n",
    "    ax[2, i+1].axis('off')\n",
    "\n",
    "# Hide any unused axes\n",
    "for j in range(3):\n",
    "    ax[j, 0].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-sample t-test\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Sample data: weights of 30 products\n",
    "sample_weights = np.array([\n",
    "    49.8, 50.2, 50.0, 49.5, 50.1, 50.3, 49.9, 50.4, 50.2, 49.7,\n",
    "    50.0, 50.1, 49.6, 50.3, 50.2, 49.8, 50.0, 50.1, 49.9, 50.2,\n",
    "    50.0, 49.7, 50.3, 50.1, 49.8, 50.0, 50.2, 49.9, 50.1, 50.0\n",
    "])\n",
    "\n",
    "# Population mean\n",
    "mu = 50.0\n",
    "\n",
    "# Perform one-sample t-test\n",
    "t_stat, p_value = stats.ttest_1samp(sample_weights, mu)\n",
    "\n",
    "print(f\"t-statistic: {t_stat:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent two-sample t-test\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Sample data: heights of 30 males and 30 females\n",
    "male_heights = np.array([\n",
    "    175, 180, 178, 182, 176, 179, 181, 177, 183, 175,\n",
    "    180, 178, 182, 176, 179, 181, 177, 183, 175, 180,\n",
    "    178, 182, 176, 179, 181, 177, 183, 175, 180, 178\n",
    "])\n",
    "\n",
    "female_heights = np.array([\n",
    "    165, 160, 162, 158, 161, 159, 163, 160, 162, 161,\n",
    "    165, 160, 162, 158, 161, 159, 163, 160, 162, 161,\n",
    "    165, 160, 162, 158, 161, 159, 163, 160, 162, 161\n",
    "])\n",
    "\n",
    "# Perform Levene's test for equal variances\n",
    "levene_stat, levene_p = stats.levene(male_heights, female_heights)\n",
    "print(f\"Levene's test p-value: {levene_p:.4f}\")\n",
    "\n",
    "# Decide whether to assume equal variances\n",
    "if levene_p > 0.05:\n",
    "    equal_var = True\n",
    "    print(\"Equal variances assumed.\")\n",
    "else:\n",
    "    equal_var = False\n",
    "    print(\"Equal variances not assumed. Using Welch's t-test.\")\n",
    "\n",
    "# Perform independent two-sample t-test\n",
    "t_stat, p_value = stats.ttest_ind(male_heights, female_heights, equal_var=equal_var)\n",
    "\n",
    "print(f\"t-statistic: {t_stat:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paired t-test\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Sample data: weights before and after a diet program for 30 individuals\n",
    "before_weights = np.array([\n",
    "    80, 82, 78, 85, 77, 79, 81, 80, 83, 78,\n",
    "    80, 82, 78, 85, 77, 79, 81, 80, 83, 78,\n",
    "    80, 82, 78, 85, 77, 79, 81, 80, 83, 78\n",
    "])\n",
    "\n",
    "after_weights = np.array([\n",
    "    78, 80, 76, 83, 75, 77, 79, 78, 81, 76,\n",
    "    78, 80, 76, 83, 75, 77, 80, 78, 81, 76,\n",
    "    78, 80, 76, 83, 75, 77, 79, 78, 81, 76\n",
    "])\n",
    "\n",
    "# Perform paired t-test\n",
    "t_stat, p_value = stats.ttest_rel(before_weights, after_weights)\n",
    "\n",
    "print(f\"t-statistic: {t_stat:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "gaussian_with_outliners = False\n",
    "uniform_and_expnential = True\n",
    "\n",
    "if gaussian_with_outliners:\n",
    "    # Define the domains for the data\n",
    "    domain_1 = np.random.normal(1.5, 0.1, 1000)  # Normal distribution with mean 1.5\n",
    "    domain_2 = np.random.normal(4.5, 0.1, 1000)  # Normal distribution with mean 2.5\n",
    "\n",
    "    # Add 10 outliers: 8 to the first domain, and 2 to the second domain\n",
    "    outliers_1 = np.full(20, 3)  # 10 outliers with value 8\n",
    "    outliers_2 = np.full(20, 3)  # 10 outliers with value 2\n",
    "\n",
    "    # Append the outliers to the domains\n",
    "    domain_1_with_outliers = np.append(domain_1, outliers_1)\n",
    "    domain_2_with_outliers = np.append(domain_2, outliers_2)\n",
    "elif uniform_and_expnential:\n",
    "    domain_1 = np.random.uniform(0.5, 1.5, 1000)  # Uniform distribution between 0.5 and 1.5\n",
    "    domain_2 = np.random.exponential(1.0, 1000)   # Exponential distribution with lambda=1.0\n",
    "\n",
    "    domain_1_with_outliers = domain_1\n",
    "    domain_2_with_outliers = domain_2\n",
    "else:\n",
    "    domain_1 = np.random.normal(1.5, 0.1, 1000)  # Normal distribution with mean 1.5\n",
    "    domain_2 = np.random.normal(4.5, 0.1, 1000)  # Normal distribution with mean 2.5\n",
    "    \n",
    "\n",
    "# Apply normalization (Min-Max Scaling)\n",
    "scaler = MinMaxScaler()\n",
    "normalized_1_with_outliers = scaler.fit_transform(domain_1_with_outliers.reshape(-1, 1)).flatten()\n",
    "normalized_2_with_outliers = scaler.fit_transform(domain_2_with_outliers.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Apply standardization (Z-score normalization)\n",
    "scaler = StandardScaler()\n",
    "standardized_1_with_outliers = scaler.fit_transform(domain_1_with_outliers.reshape(-1, 1)).flatten()\n",
    "standardized_2_with_outliers = scaler.fit_transform(domain_2_with_outliers.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Plot the original, normalized, and standardized data with 10 outliers\n",
    "fig, ax = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Original data with 10 outliers\n",
    "ax[0, 0].hist(domain_1_with_outliers, bins=30, color='blue', alpha=0.7, label='Domain 1 (Original + 10 Outliers)')\n",
    "ax[0, 0].hist(domain_2_with_outliers, bins=30, color='green', alpha=0.7, label='Domain 2 (Original + 10 Outliers)')\n",
    "ax[0, 0].set_title(\"Original Data with 10 Outliers\")\n",
    "ax[0, 0].legend()\n",
    "\n",
    "# Normalized data with 10 outliers\n",
    "ax[0, 1].hist(normalized_1_with_outliers, bins=30, color='blue', alpha=0.7, label='Domain 1 (Normalized + 10 Outliers)')\n",
    "ax[0, 1].hist(normalized_2_with_outliers, bins=30, color='green', alpha=0.7, label='Domain 2 (Normalized + 10 Outliers)')\n",
    "ax[0, 1].set_title(\"Normalized Data with 10 Outliers\")\n",
    "ax[0, 1].legend()\n",
    "\n",
    "# Standardized data with 10 outliers\n",
    "ax[1, 0].hist(standardized_1_with_outliers, bins=30, color='blue', alpha=0.7, label='Domain 1 (Standardized + 10 Outliers)')\n",
    "ax[1, 0].hist(standardized_2_with_outliers, bins=30, color='green', alpha=0.7, label='Domain 2 (Standardized + 10 Outliers)')\n",
    "ax[1, 0].set_title(\"Standardized Data with 10 Outliers\")\n",
    "ax[1, 0].legend()\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(0)\n",
    "data = np.sin(np.linspace(0, 10, 100)) * 10 + np.random.normal(20, 5, 100)\n",
    "\n",
    "# Standardization\n",
    "scaler_standard = StandardScaler()\n",
    "data_standardized = scaler_standard.fit_transform(data.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Normalization\n",
    "scaler_minmax = MinMaxScaler()\n",
    "data_normalized = scaler_minmax.fit_transform(data.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Original vs Standardized vs Normalized Data\")\n",
    "plt.plot(data, label='Original', alpha=0.7)\n",
    "plt.plot(data_standardized, label='Standardized', alpha=0.7)\n",
    "plt.plot(data_normalized, label='Normalized', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.ylabel('Value')\n",
    "plt.xlabel('Index')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Data Distribution\")\n",
    "plt.hist(data, bins=20, alpha=0.5, label='Original')\n",
    "plt.hist(data_standardized, bins=20, alpha=0.5, label='Standardized')\n",
    "plt.hist(data_normalized, bins=20, alpha=0.5, label='Normalized')\n",
    "plt.legend()\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print some statistics\n",
    "print(\"Original Data - Mean: {:.2f}, Std: {:.2f}, Min: {:.2f}, Max: {:.2f}\".format(\n",
    "    np.mean(data), np.std(data), np.min(data), np.max(data)))\n",
    "print(\"Standardized Data - Mean: {:.2f}, Std: {:.2f}, Min: {:.2f}, Max: {:.2f}\".format(\n",
    "    np.mean(data_standardized), np.std(data_standardized), np.min(data_standardized), np.max(data_standardized)))\n",
    "print(\"Normalized Data - Mean: {:.2f}, Std: {:.2f}, Min: {:.2f}, Max: {:.2f}\".format(\n",
    "    np.mean(data_normalized), np.std(data_normalized), np.min(data_normalized), np.max(data_normalized)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "\n",
    "all_densenet_models = timm.list_models('*resnet*', pretrained=True)\n",
    "display(all_densenet_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "data = {\n",
    "    'Color': ['Red', 'Blue', 'Green', 'Red', None, 'Blue', 'Red', None],\n",
    "    'Size': [10, 15, None, 14, 13, 10, None, 12],\n",
    "    'Price': [100, None, 150, 125, None, 110, 130, None]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "numerical_features = ['Size', 'Price']\n",
    "categorical_features = ['Color']\n",
    "\n",
    "# Imputer for nume\n",
    "numerical_transformer = SimpleImputer(strategy='median')\n",
    "# Imputer for cat\n",
    "categorical_transformer = SimpleImputer(strategy='most_frequent', missing_values=None)\n",
    "\n",
    "# apply different transformations to different columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply\n",
    "df_transformed = preprocessor.fit_transform(df)\n",
    "\n",
    "# Convert the transformed array back to a DataFrame\n",
    "df_transformed = pd.DataFrame(df_transformed, columns=numerical_features + categorical_features)\n",
    "\n",
    "print(\"\\nDataFrame after imputation:\")\n",
    "print(df_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import mode\n",
    "\n",
    "# Sample data\n",
    "data = {'A': [1, 2, 2, None, 2, 3, 4, 4, 5],\n",
    "        'B': ['red', 'blue', None, 'blue', 'red', 'red', 'red', 'green', 'blue']}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Function to apply mode imputation\n",
    "def mode_imputation(df):\n",
    "    for column in df.columns:\n",
    "        # Calculate the mode of the column\n",
    "        most_frequent = mode(df[column]).mode[0]\n",
    "        # Replace missing values with the mode\n",
    "        df[column].fillna(most_frequent, inplace=True)\n",
    "    return df\n",
    "\n",
    "# Apply mode imputation to the dataframe\n",
    "imputed_df = mode_imputation(df)\n",
    "\n",
    "print(imputed_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Sample dataset\n",
    "data = {\n",
    "    'Color': ['Red', 'Blue', 'Green', 'Red', None, 'Blue', 'Red', None]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the original data\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Initialize the SimpleImputer with strategy='most_frequent' for mode imputation\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Perform mode imputation on the 'Color' column\n",
    "df['Color'] = imputer.fit_transform(df[['Color']])\n",
    "\n",
    "# Display the DataFrame after mode imputation\n",
    "print(\"\\nDataFrame after mode imputation:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = ['就寝', '起床']\n",
    "for elem in aa:\n",
    "    if elem in '起床就　寝aa':\n",
    "        print(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "import timm\n",
    "\n",
    "img = Image.open(urlopen(\n",
    "    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n",
    "))\n",
    "\n",
    "model = timm.create_model(\n",
    "    'resnet18.a1_in1k',\n",
    "    # \"efficientnet_b0.ra_in1k\",\n",
    "    pretrained=True,\n",
    "    features_only=True,\n",
    ")\n",
    "model = model.eval()\n",
    "\n",
    "# get model specific transforms (normalization, resize)\n",
    "data_config = timm.data.resolve_model_data_config(model)\n",
    "transforms = timm.data.create_transform(**data_config, is_training=False)\n",
    "\n",
    "output = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n",
    "\n",
    "for o in output:\n",
    "    # print shape of each feature map in output\n",
    "    # e.g.:\n",
    "    #  torch.Size([1, 64, 112, 112])\n",
    "    #  torch.Size([1, 64, 56, 56])\n",
    "    #  torch.Size([1, 128, 28, 28])\n",
    "    #  torch.Size([1, 256, 14, 14])\n",
    "    #  torch.Size([1, 512, 7, 7])\n",
    "\n",
    "    print(o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "# Generate data\n",
    "x = np.arange(1, 101)  # Simple increasing sequence: 1 to 100\n",
    "y_linear = x  # Linear relationship\n",
    "\n",
    "# Generate a sine wave added to the linearly increasing sequence\n",
    "y_sin_wave = np.sin(np.linspace(0, 50 * np.pi, 100)) + x  # Sine wave with increasing trend\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, y_linear, 'o-', label='Linear (1 to 100)')\n",
    "plt.title(\"Linear Data (1 to 100)\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x, y_sin_wave, 'o-', label='Sine Wave + Linear')\n",
    "plt.title(\"Sine Wave + Linear Data\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate Pearson and Spearman correlations\n",
    "pearson_corr_linear, _ = pearsonr(x, y_linear)\n",
    "spearman_corr_linear, _ = spearmanr(x, y_linear)\n",
    "\n",
    "pearson_corr_sin_wave, _ = pearsonr(x, y_sin_wave)\n",
    "spearman_corr_sin_wave, _ = spearmanr(x, y_sin_wave)\n",
    "\n",
    "print(\"Linear Data (1 to 100):\")\n",
    "print(f\"Pearson Correlation Coefficient: {pearson_corr_linear:.2f}\")\n",
    "print(f\"Spearman's Rank Correlation: {spearman_corr_linear:.2f}\")\n",
    "print()\n",
    "\n",
    "print(\"Sine Wave + Linear Data:\")\n",
    "print(f\"Pearson Correlation Coefficient: {pearson_corr_sin_wave:.2f}\")\n",
    "print(f\"Spearman's Rank Correlation: {spearman_corr_sin_wave:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Example data: x vs. y (non-linear, monotonic relationship)\n",
    "x = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "y = np.array([2, 4, 8, 16, 32, 64, 128, 256, 512, 1024])\n",
    "\n",
    "# Plot the data\n",
    "plt.scatter(x, y)\n",
    "plt.title(\"Original Data\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Calculate Correlation\n",
    "correlation_coefficient, p_value = pearsonr(x, y)\n",
    "spearman_corr, p_value = spearmanr(x, y)\n",
    "\n",
    "\n",
    "print(f\"Pearson Correlation Coefficient (log-transformed): {correlation_coefficient:.2f}\")\n",
    "print(f\"P-value: {p_value:.2f}\")\n",
    "\n",
    "print(f\"Spearman's Rank Correlation: {spearman_corr:.2f}\")\n",
    "print(f\"P-value: {p_value:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Example data: x vs. y (non-linear relationship)\n",
    "x = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "y = np.array([2, 4, 8, 16, 32, 64, 128, 256, 512, 1024])\n",
    "\n",
    "# Original data plot\n",
    "plt.scatter(x, y)\n",
    "plt.title(\"Original Data\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()\n",
    "\n",
    "# Apply a logarithmic transformation to y\n",
    "log_y = np.log(y)\n",
    "\n",
    "# Transformed data plot\n",
    "plt.scatter(x, log_y)\n",
    "plt.title(\"Log-Transformed Data\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"log(y)\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate Pearson Correlation on transformed data\n",
    "correlation_coefficient, p_value = pearsonr(x, y)\n",
    "\n",
    "print(f\"Pearson Correlation Coefficient (log-transformed): {correlation_coefficient:.2f}\")\n",
    "print(f\"P-value: {p_value:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (1,2,3)\n",
    "\n",
    "def funca():\n",
    "    return a\n",
    "\n",
    "e,b = funca()\n",
    "print(e,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "import xgboost as xgb\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Example data: hours studied vs. exam scores\n",
    "hours_studied = np.array([1, 2, 3, 4, 3.5])\n",
    "exam_scores = np.array([60, 70, 75, 85, 90])\n",
    "\n",
    "# Calculate Pearson Correlation Coefficient\n",
    "correlation_coefficient, p_value = pearsonr(hours_studied, exam_scores)\n",
    "\n",
    "print(f\"Pearson Correlation Coefficient: {correlation_coefficient:.2f}\")\n",
    "print(f\"P-value: {p_value:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a sample of data\n",
    "np.random.seed(42)\n",
    "sample_data = np.random.normal(loc=0, scale=1, size=100)  # A sample from a normal distribution\n",
    "\n",
    "# Perform the K-S test against a normal distribution\n",
    "D, p_value = stats.kstest(sample_data, 'norm', args=(0, 1))\n",
    "\n",
    "print(f\"KS Statistic: {D}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "# Interpretation\n",
    "if p_value < 0.05:\n",
    "    print(\"Reject the null hypothesis: The sample does not follow a normal distribution.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: The sample follows a normal distribution.\")\n",
    "\n",
    "# Plotting the empirical CDF vs the theoretical CDF\n",
    "ecdf = np.sort(sample_data)\n",
    "cdf = np.arange(1, len(ecdf) + 1) / len(ecdf)\n",
    "plt.plot(ecdf, cdf, marker='.', linestyle='none', label='Empirical CDF')\n",
    "\n",
    "# Theoretical CDF for the normal distribution\n",
    "x = np.linspace(min(ecdf), max(ecdf), 100)\n",
    "plt.plot(x, stats.norm.cdf(x, loc=0, scale=1), label='Theoretical CDF (Normal Dist)')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('CDF')\n",
    "plt.title('Empirical CDF vs Theoretical CDF')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize individual models\n",
    "model1 = LogisticRegression()\n",
    "model2 = DecisionTreeClassifier()\n",
    "model3 = SVC(probability=True)\n",
    "\n",
    "# Initialize Voting Classifier\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('lr', model1), ('dt', model2), ('svc', model3)],\n",
    "    voting='soft',\n",
    "    weights=[0.3, 0.3, 0.3])  # Use 'hard' for hard voting\n",
    "\n",
    "# Train and predict\n",
    "voting_clf.fit(X_train, y_train)\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "display(y_test)\n",
    "display(y_pred)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path('/kaggle/input/isic-2024-challenge')\n",
    "\n",
    "train_path = '/root/Development/Kaggle/ISIC2024/data/raw/train-metadata.csv'\n",
    "test_path = '/root/Development/Kaggle/ISIC2024/data/raw/test-metadata.csv'\n",
    "subm_path = '/root/Development/Kaggle/ISIC2024/data/raw/sample_submission.csv'\n",
    "\n",
    "id_col = 'isic_id'\n",
    "target_col = 'target'\n",
    "group_col = 'patient_id'\n",
    "\n",
    "err = 1e-5\n",
    "sampling_ratio = 0.01\n",
    "seed = 42\n",
    "\n",
    "num_cols = [\n",
    "    'age_approx',                        # Approximate age of patient at time of imaging.\n",
    "    'clin_size_long_diam_mm',            # Maximum diameter of the lesion (mm).+\n",
    "    'tbp_lv_A',                          # A inside  lesion.+\n",
    "    'tbp_lv_Aext',                       # A outside lesion.+\n",
    "    'tbp_lv_B',                          # B inside  lesion.+\n",
    "    'tbp_lv_Bext',                       # B outside lesion.+ \n",
    "    'tbp_lv_C',                          # Chroma inside  lesion.+\n",
    "    'tbp_lv_Cext',                       # Chroma outside lesion.+\n",
    "    'tbp_lv_H',                          # Hue inside the lesion; calculated as the angle of A* and B* in LAB* color space. Typical values range from 25 (red) to 75 (brown).+\n",
    "    'tbp_lv_Hext',                       # Hue outside lesion.+\n",
    "    'tbp_lv_L',                          # L inside lesion.+\n",
    "    'tbp_lv_Lext',                       # L outside lesion.+\n",
    "    'tbp_lv_areaMM2',                    # Area of lesion (mm^2).+\n",
    "    'tbp_lv_area_perim_ratio',           # Border jaggedness, the ratio between lesions perimeter and area. Circular lesions will have low values; irregular shaped lesions will have higher values. Values range 0-10.+\n",
    "    'tbp_lv_color_std_mean',             # Color irregularity, calculated as the variance of colors within the lesion's boundary.\n",
    "    'tbp_lv_deltaA',                     # Average A contrast (inside vs. outside lesion).+\n",
    "    'tbp_lv_deltaB',                     # Average B contrast (inside vs. outside lesion).+\n",
    "    'tbp_lv_deltaL',                     # Average L contrast (inside vs. outside lesion).+\n",
    "    'tbp_lv_deltaLB',                    #\n",
    "    'tbp_lv_deltaLBnorm',                # Contrast between the lesion and its immediate surrounding skin. Low contrast lesions tend to be faintly visible such as freckles; high contrast lesions tend to be those with darker pigment. Calculated as the average delta LB of the lesion relative to its immediate background in LAB* color space. Typical values range from 5.5 to 25.+\n",
    "    'tbp_lv_eccentricity',               # Eccentricity.+\n",
    "    'tbp_lv_minorAxisMM',                # Smallest lesion diameter (mm).+\n",
    "    'tbp_lv_nevi_confidence',            # Nevus confidence score (0-100 scale) is a convolutional neural network classifier estimated probability that the lesion is a nevus. The neural network was trained on approximately 57,000 lesions that were classified and labeled by a dermatologist.+,++\n",
    "    'tbp_lv_norm_border',                # Border irregularity (0-10 scale); the normalized average of border jaggedness and asymmetry.+\n",
    "    'tbp_lv_norm_color',                 # Color variation (0-10 scale); the normalized average of color asymmetry and color irregularity.+\n",
    "    'tbp_lv_perimeterMM',                # Perimeter of lesion (mm).+\n",
    "    'tbp_lv_radial_color_std_max',       # Color asymmetry, a measure of asymmetry of the spatial distribution of color within the lesion. This score is calculated by looking at the average standard deviation in LAB* color space within concentric rings originating from the lesion center. Values range 0-10.+\n",
    "    'tbp_lv_stdL',                       # Standard deviation of L inside  lesion.+\n",
    "    'tbp_lv_stdLExt',                    # Standard deviation of L outside lesion.+\n",
    "    'tbp_lv_symm_2axis',                 # Border asymmetry; a measure of asymmetry of the lesion's contour about an axis perpendicular to the lesion's most symmetric axis. Lesions with two axes of symmetry will therefore have low scores (more symmetric), while lesions with only one or zero axes of symmetry will have higher scores (less symmetric). This score is calculated by comparing opposite halves of the lesion contour over many degrees of rotation. The angle where the halves are most similar identifies the principal axis of symmetry, while the second axis of symmetry is perpendicular to the principal axis. Border asymmetry is reported as the asymmetry value about this second axis. Values range 0-10.+\n",
    "    'tbp_lv_symm_2axis_angle',           # Lesion border asymmetry angle.+\n",
    "    'tbp_lv_x',                          # X-coordinate of the lesion on 3D TBP.+\n",
    "    'tbp_lv_y',                          # Y-coordinate of the lesion on 3D TBP.+\n",
    "    'tbp_lv_z',                          # Z-coordinate of the lesion on 3D TBP.+\n",
    "]\n",
    "\n",
    "new_num_cols = [\n",
    "    'lesion_size_ratio',             # tbp_lv_minorAxisMM      / clin_size_long_diam_mm\n",
    "    'lesion_shape_index',            # tbp_lv_areaMM2          / tbp_lv_perimeterMM **2\n",
    "    'hue_contrast',                  # tbp_lv_H                - tbp_lv_Hext              abs\n",
    "    'luminance_contrast',            # tbp_lv_L                - tbp_lv_Lext              abs\n",
    "    'lesion_color_difference',       # tbp_lv_deltaA **2       + tbp_lv_deltaB **2 + tbp_lv_deltaL **2  sqrt  \n",
    "    'border_complexity',             # tbp_lv_norm_border      + tbp_lv_symm_2axis\n",
    "    'color_uniformity',              # tbp_lv_color_std_mean   / tbp_lv_radial_color_std_max\n",
    "\n",
    "    'position_distance_3d',          # tbp_lv_x **2 + tbp_lv_y **2 + tbp_lv_z **2  sqrt\n",
    "    'perimeter_to_area_ratio',       # tbp_lv_perimeterMM      / tbp_lv_areaMM2\n",
    "    'area_to_perimeter_ratio',       # tbp_lv_areaMM2          / tbp_lv_perimeterMM\n",
    "    'lesion_visibility_score',       # tbp_lv_deltaLBnorm      + tbp_lv_norm_color\n",
    "    'symmetry_border_consistency',   # tbp_lv_symm_2axis       * tbp_lv_norm_border\n",
    "    'consistency_symmetry_border',   # tbp_lv_symm_2axis       * tbp_lv_norm_border / (tbp_lv_symm_2axis + tbp_lv_norm_border)\n",
    "\n",
    "    'color_consistency',             # tbp_lv_stdL             / tbp_lv_Lext\n",
    "    'consistency_color',             # tbp_lv_stdL*tbp_lv_Lext / tbp_lv_stdL + tbp_lv_Lext\n",
    "    'size_age_interaction',          # clin_size_long_diam_mm  * age_approx\n",
    "    'hue_color_std_interaction',     # tbp_lv_H                * tbp_lv_color_std_mean\n",
    "    'lesion_severity_index',         # tbp_lv_norm_border      + tbp_lv_norm_color + tbp_lv_eccentricity / 3\n",
    "    'shape_complexity_index',        # border_complexity       + lesion_shape_index\n",
    "    'color_contrast_index',          # tbp_lv_deltaA + tbp_lv_deltaB + tbp_lv_deltaL + tbp_lv_deltaLBnorm\n",
    "\n",
    "    'log_lesion_area',               # tbp_lv_areaMM2          + 1  np.log\n",
    "    'normalized_lesion_size',        # clin_size_long_diam_mm  / age_approx\n",
    "    'mean_hue_difference',           # tbp_lv_H                + tbp_lv_Hext    / 2\n",
    "    'std_dev_contrast',              # tbp_lv_deltaA **2 + tbp_lv_deltaB **2 + tbp_lv_deltaL **2   / 3  np.sqrt\n",
    "    'color_shape_composite_index',   # tbp_lv_color_std_mean   + bp_lv_area_perim_ratio + tbp_lv_symm_2axis   / 3\n",
    "    'lesion_orientation_3d',         # tbp_lv_y                , tbp_lv_x  np.arctan2\n",
    "    'overall_color_difference',      # tbp_lv_deltaA           + tbp_lv_deltaB + tbp_lv_deltaL   / 3\n",
    "\n",
    "    'symmetry_perimeter_interaction',# tbp_lv_symm_2axis       * tbp_lv_perimeterMM\n",
    "    'comprehensive_lesion_index',    # tbp_lv_area_perim_ratio + tbp_lv_eccentricity + bp_lv_norm_color + tbp_lv_symm_2axis   / 4\n",
    "    'color_variance_ratio',          # tbp_lv_color_std_mean   / tbp_lv_stdLExt\n",
    "    'border_color_interaction',      # tbp_lv_norm_border      * tbp_lv_norm_color\n",
    "    'border_color_interaction_2',\n",
    "    'size_color_contrast_ratio',     # clin_size_long_diam_mm  / tbp_lv_deltaLBnorm\n",
    "    'age_normalized_nevi_confidence',# tbp_lv_nevi_confidence  / age_approx\n",
    "    'age_normalized_nevi_confidence_2',\n",
    "    'color_asymmetry_index',         # tbp_lv_symm_2axis       * tbp_lv_radial_color_std_max\n",
    "\n",
    "    'volume_approximation_3d',       # tbp_lv_areaMM2          * sqrt(tbp_lv_x**2 + tbp_lv_y**2 + tbp_lv_z**2)\n",
    "    'color_range',                   # abs(tbp_lv_L - tbp_lv_Lext) + abs(tbp_lv_A - tbp_lv_Aext) + abs(tbp_lv_B - tbp_lv_Bext)\n",
    "    'shape_color_consistency',       # tbp_lv_eccentricity     * tbp_lv_color_std_mean\n",
    "    'border_length_ratio',           # tbp_lv_perimeterMM      / pi * sqrt(tbp_lv_areaMM2 / pi)\n",
    "    'age_size_symmetry_index',       # age_approx              * clin_size_long_diam_mm * tbp_lv_symm_2axis\n",
    "    'index_age_size_symmetry',       # age_approx              * tbp_lv_areaMM2 * tbp_lv_symm_2axis\n",
    "]\n",
    "\n",
    "cat_cols = ['sex', 'anatom_site_general', 'tbp_tile_type', 'tbp_lv_location', 'tbp_lv_location_simple', 'attribution']\n",
    "norm_cols = [f'{col}_patient_norm' for col in num_cols + new_num_cols]\n",
    "special_cols = ['count_per_patient']\n",
    "feature_cols = num_cols + new_num_cols + cat_cols + norm_cols + special_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    return (\n",
    "        # pl.read_csv(path, n_rows=10000)\n",
    "        pl.read_csv(path)\n",
    "        .with_columns(\n",
    "            pl.col('age_approx').cast(pl.String).replace('NA', np.nan).cast(pl.Float64),\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl.col(pl.Float64).fill_nan(pl.col(pl.Float64).median()), # You may want to impute test data with train\n",
    "        )\n",
    "        .with_columns(\n",
    "            lesion_size_ratio              = pl.col('tbp_lv_minorAxisMM') / pl.col('clin_size_long_diam_mm'),\n",
    "            lesion_shape_index             = pl.col('tbp_lv_areaMM2') / (pl.col('tbp_lv_perimeterMM') ** 2),\n",
    "            hue_contrast                   = (pl.col('tbp_lv_H') - pl.col('tbp_lv_Hext')).abs(),\n",
    "            luminance_contrast             = (pl.col('tbp_lv_L') - pl.col('tbp_lv_Lext')).abs(),\n",
    "            lesion_color_difference        = (pl.col('tbp_lv_deltaA') ** 2 + pl.col('tbp_lv_deltaB') ** 2 + pl.col('tbp_lv_deltaL') ** 2).sqrt(),\n",
    "            border_complexity              = pl.col('tbp_lv_norm_border') + pl.col('tbp_lv_symm_2axis'),\n",
    "            color_uniformity               = pl.col('tbp_lv_color_std_mean') / (pl.col('tbp_lv_radial_color_std_max') + err),\n",
    "        )\n",
    "        .with_columns(\n",
    "            position_distance_3d           = (pl.col('tbp_lv_x') ** 2 + pl.col('tbp_lv_y') ** 2 + pl.col('tbp_lv_z') ** 2).sqrt(),\n",
    "            perimeter_to_area_ratio        = pl.col('tbp_lv_perimeterMM') / pl.col('tbp_lv_areaMM2'),\n",
    "            area_to_perimeter_ratio        = pl.col('tbp_lv_areaMM2') / pl.col('tbp_lv_perimeterMM'),\n",
    "            lesion_visibility_score        = pl.col('tbp_lv_deltaLBnorm') + pl.col('tbp_lv_norm_color'),\n",
    "            combined_anatomical_site       = pl.col('anatom_site_general') + '_' + pl.col('tbp_lv_location'),\n",
    "            symmetry_border_consistency    = pl.col('tbp_lv_symm_2axis') * pl.col('tbp_lv_norm_border'),\n",
    "            consistency_symmetry_border    = pl.col('tbp_lv_symm_2axis') * pl.col('tbp_lv_norm_border') / (pl.col('tbp_lv_symm_2axis') + pl.col('tbp_lv_norm_border')),\n",
    "        )\n",
    "        .with_columns(\n",
    "            color_consistency              = pl.col('tbp_lv_stdL') / pl.col('tbp_lv_Lext'),\n",
    "            consistency_color              = pl.col('tbp_lv_stdL') * pl.col('tbp_lv_Lext') / (pl.col('tbp_lv_stdL') + pl.col('tbp_lv_Lext')),\n",
    "            size_age_interaction           = pl.col('clin_size_long_diam_mm') * pl.col('age_approx'),\n",
    "            hue_color_std_interaction      = pl.col('tbp_lv_H') * pl.col('tbp_lv_color_std_mean'),\n",
    "            lesion_severity_index          = (pl.col('tbp_lv_norm_border') + pl.col('tbp_lv_norm_color') + pl.col('tbp_lv_eccentricity')) / 3,\n",
    "            shape_complexity_index         = pl.col('border_complexity') + pl.col('lesion_shape_index'),\n",
    "            color_contrast_index           = pl.col('tbp_lv_deltaA') + pl.col('tbp_lv_deltaB') + pl.col('tbp_lv_deltaL') + pl.col('tbp_lv_deltaLBnorm'),\n",
    "        )\n",
    "        .with_columns(\n",
    "            log_lesion_area                = (pl.col('tbp_lv_areaMM2') + 1).log(),\n",
    "            normalized_lesion_size         = pl.col('clin_size_long_diam_mm') / pl.col('age_approx'),\n",
    "            mean_hue_difference            = (pl.col('tbp_lv_H') + pl.col('tbp_lv_Hext')) / 2,\n",
    "            std_dev_contrast               = ((pl.col('tbp_lv_deltaA') ** 2 + pl.col('tbp_lv_deltaB') ** 2 + pl.col('tbp_lv_deltaL') ** 2) / 3).sqrt(),\n",
    "            color_shape_composite_index    = (pl.col('tbp_lv_color_std_mean') + pl.col('tbp_lv_area_perim_ratio') + pl.col('tbp_lv_symm_2axis')) / 3,\n",
    "            lesion_orientation_3d          = pl.arctan2(pl.col('tbp_lv_y'), pl.col('tbp_lv_x')),\n",
    "            overall_color_difference       = (pl.col('tbp_lv_deltaA') + pl.col('tbp_lv_deltaB') + pl.col('tbp_lv_deltaL')) / 3,\n",
    "        )\n",
    "        .with_columns(\n",
    "            symmetry_perimeter_interaction = pl.col('tbp_lv_symm_2axis') * pl.col('tbp_lv_perimeterMM'),\n",
    "            comprehensive_lesion_index     = (pl.col('tbp_lv_area_perim_ratio') + pl.col('tbp_lv_eccentricity') + pl.col('tbp_lv_norm_color') + pl.col('tbp_lv_symm_2axis')) / 4,\n",
    "            color_variance_ratio           = pl.col('tbp_lv_color_std_mean') / pl.col('tbp_lv_stdLExt'),\n",
    "            border_color_interaction       = pl.col('tbp_lv_norm_border') * pl.col('tbp_lv_norm_color'),\n",
    "            border_color_interaction_2     = pl.col('tbp_lv_norm_border') * pl.col('tbp_lv_norm_color') / (pl.col('tbp_lv_norm_border') + pl.col('tbp_lv_norm_color')),\n",
    "            size_color_contrast_ratio      = pl.col('clin_size_long_diam_mm') / pl.col('tbp_lv_deltaLBnorm'),\n",
    "            age_normalized_nevi_confidence = pl.col('tbp_lv_nevi_confidence') / pl.col('age_approx'),\n",
    "            age_normalized_nevi_confidence_2 = (pl.col('clin_size_long_diam_mm')**2 + pl.col('age_approx')**2).sqrt(),\n",
    "            color_asymmetry_index          = pl.col('tbp_lv_radial_color_std_max') * pl.col('tbp_lv_symm_2axis'),\n",
    "        )\n",
    "        .with_columns(\n",
    "            volume_approximation_3d        = pl.col('tbp_lv_areaMM2') * (pl.col('tbp_lv_x')**2 + pl.col('tbp_lv_y')**2 + pl.col('tbp_lv_z')**2).sqrt(),\n",
    "            color_range                    = (pl.col('tbp_lv_L') - pl.col('tbp_lv_Lext')).abs() + (pl.col('tbp_lv_A') - pl.col('tbp_lv_Aext')).abs() + (pl.col('tbp_lv_B') - pl.col('tbp_lv_Bext')).abs(),\n",
    "            shape_color_consistency        = pl.col('tbp_lv_eccentricity') * pl.col('tbp_lv_color_std_mean'),\n",
    "            border_length_ratio            = pl.col('tbp_lv_perimeterMM') / (2 * np.pi * (pl.col('tbp_lv_areaMM2') / np.pi).sqrt()),\n",
    "            age_size_symmetry_index        = pl.col('age_approx') * pl.col('clin_size_long_diam_mm') * pl.col('tbp_lv_symm_2axis'),\n",
    "            index_age_size_symmetry        = pl.col('age_approx') * pl.col('tbp_lv_areaMM2') * pl.col('tbp_lv_symm_2axis'),\n",
    "        )\n",
    "        # .with_columns(\n",
    "        #     ((pl.col(col) - pl.col(col).mean().over('patient_id')) / (pl.col(col).std().over('patient_id') + err)).alias(f'{col}_patient_norm') for col in (num_cols + new_num_cols)\n",
    "        # )\n",
    "        .with_columns(\n",
    "            count_per_patient = pl.col('isic_id').count().over('patient_id'),\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl.col(cat_cols).cast(pl.Categorical),\n",
    "        )\n",
    "        .to_pandas()\n",
    "        .set_index(id_col)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df_train, df_test):\n",
    "    global cat_cols\n",
    "    \n",
    "    encoder = OneHotEncoder(sparse_output=False, dtype=np.int32, handle_unknown='ignore')\n",
    "    encoder.fit(df_train[cat_cols])\n",
    "    \n",
    "    new_cat_cols = [f'onehot_{i}' for i in range(len(encoder.get_feature_names_out()))]\n",
    "\n",
    "    df_train[new_cat_cols] = encoder.transform(df_train[cat_cols])\n",
    "    df_train[new_cat_cols] = df_train[new_cat_cols].astype('category')\n",
    "\n",
    "    df_test[new_cat_cols] = encoder.transform(df_test[cat_cols])\n",
    "    df_test[new_cat_cols] = df_test[new_cat_cols].astype('category')\n",
    "\n",
    "    for col in cat_cols:\n",
    "        feature_cols.remove(col)\n",
    "\n",
    "    feature_cols.extend(new_cat_cols)\n",
    "    cat_cols = new_cat_cols\n",
    "    \n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_metric(estimator, X, y_true):\n",
    "    y_hat = estimator.predict_proba(X)[:, 1]\n",
    "    min_tpr = 0.80\n",
    "    max_fpr = abs(1 - min_tpr)\n",
    "    \n",
    "    v_gt = abs(y_true - 1)\n",
    "    v_pred = np.array([1.0 - x for x in y_hat])\n",
    "    \n",
    "    partial_auc_scaled = roc_auc_score(v_gt, v_pred, max_fpr=max_fpr)\n",
    "    partial_auc = 0.5 * max_fpr**2 + (max_fpr - 0.5 * max_fpr**2) / (1.0 - 0.5) * (partial_auc_scaled - 0.5)\n",
    "    \n",
    "    return partial_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = read_data(train_path)\n",
    "new_columns = {}\n",
    "for col in num_cols + new_num_cols:\n",
    "    patient_mean = df_train.groupby('patient_id')[col].transform('mean')\n",
    "    patient_std = df_train.groupby('patient_id')[col].transform('std')\n",
    "    # Store the normalized column in the dictionary\n",
    "    new_columns[f'{col}_patient_norm'] = (df_train[col] - patient_mean) / (patient_std + err)\n",
    "df_train = pd.concat([df_train, pd.DataFrame(new_columns)], axis=1)\n",
    "\n",
    "\n",
    "df_test = read_data(test_path)\n",
    "new_columns = {}\n",
    "for col in num_cols + new_num_cols:\n",
    "    patient_mean_test = df_test.groupby('patient_id')[col].transform('mean')\n",
    "    patient_std_test = df_test.groupby('patient_id')[col].transform('std')\n",
    "    # Store the normalized column in the dictionary\n",
    "    new_columns[f'{col}_patient_norm'] = (df_test[col] - patient_mean_test) / (patient_std_test + err)\n",
    "df_test = pd.concat([df_test, pd.DataFrame(new_columns)], axis=1)\n",
    "\n",
    "# .with_columns(\n",
    "        #     ((pl.col(col) - pl.col(col).mean().over('patient_id')) / (pl.col(col).std().over('patient_id') + err)).alias(f'{col}_patient_norm') for col in (num_cols + new_num_cols)\n",
    "        # )\n",
    "\n",
    "\n",
    "df_subm = pd.read_csv(subm_path, index_col=id_col)\n",
    "\n",
    "df_train, df_test = preprocess(df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_train)\n",
    "display(df_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
